terraform {
  required_version = ">= 1.0"

backend "s3" {
    bucket         = "travsim-terraform-state"
    key            = "eks-prod/terraform.tfstate"
    region         = "ap-southeast-1"
    dynamodb_table = "terraform-state-lock"
    encrypt        = true

    # ⚠️ REQUIRED: Tells the backend to use the role to access S3
    role_arn       = "arn:aws:iam::135384223494:role/TerraformProvisioningRole"
  }

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 5.95.0"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.9"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.20"
    }
  }
}

provider "aws" {
  region = "ap-southeast-1"

  assume_role {
    # The ARN of the role you created (e.g., TerraformProvisioningRole)
    role_arn     = "arn:aws:iam::135384223494:role/TerraformProvisioningRole"
    session_name = "TerraformProviderSession"
  }
}

# Providers rely on the EKS module output for auth
provider "kubernetes" {
  host                   = data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority[0].data)
  
  # This uses the token generated by the assumed role
  token                  = data.aws_eks_cluster_auth.cluster.token
}

provider "helm" {
  kubernetes {
    host                   = data.aws_eks_cluster.cluster.endpoint
    cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority[0].data)
    token                  = data.aws_eks_cluster_auth.cluster.token
  }
}

# Fetch the cluster details
data "aws_eks_cluster" "cluster" {
  name = module.eks.cluster_name
  # dependent on the module finishing first
  depends_on = [module.eks] 
}

# Fetch a fresh authentication token
data "aws_eks_cluster_auth" "cluster" {
  name = module.eks.cluster_name
  depends_on = [module.eks]
}